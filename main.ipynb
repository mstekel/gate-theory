{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1UXMnW7Fd1cKIxYM5WbDRMuQw8OS7kxtG",
      "authorship_tag": "ABX9TyNVeeMckISChUQti4g6+qAV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mstekel/gate-theory/blob/main/akk_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ3TbAIhjXuO"
      },
      "source": [
        "!pip3 install text-fabric\n",
        "!pip3 install gensim==4.1.2 --user\n",
        "!pip3 install jsonpath-ng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzP-xUYCjpab"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"PYTHONHASHSEED\"] = \"777\"\n",
        "import logging  # Setting up the loggings to monitor gensim\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLRnPeBYjx5C"
      },
      "source": [
        "from gensim import models\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from os import path\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import sys\n",
        "\n",
        "if platform.system() == 'Windows':\n",
        "    os.environ[\"DRIVE_ROOT\"] = str(Path(os.environ[\"USERPROFILE\"]) / \"Google Drive\")\n",
        "else:\n",
        "    os.environ[\"DRIVE_ROOT\"] = '/content/drive/MyDrive'\n",
        "DRIVE_ROOT = Path(os.environ[\"DRIVE_ROOT\"])\n",
        "sys.path.append(str(DRIVE_ROOT / 'Colab Notebooks/akk_word2vec'))\n",
        "\n",
        "from bible import config as bible_config\n",
        "#from oracc import config as oracc_config\n",
        "#from simpsons import config as simpsons_config\n",
        "\n",
        "#config = simpsons_config\n",
        "config = bible_config\n",
        "#config = oracc_config\n",
        "\n",
        "window = config['word2vec_args']['window']\n",
        "epochs = config['epochs']\n",
        "\n",
        "pos = ['verb','subs','advb','adjv','nmpr']\n",
        "# pos = []\n",
        "\n",
        "model_path = str(\n",
        "    DRIVE_ROOT\n",
        "    / f\"models/word2vec/{config['corpus']}_{window}_{epochs}_{pos}.model\"\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    config[\"corpus_ctor\"](pos),\n",
        "    columns= [\n",
        "        'origin',\n",
        "        'context', \n",
        "        'clean',\n",
        "        'sense',\n",
        "        'genre'\n",
        "    ]\n",
        ")\n",
        "\n",
        "display(df)\n",
        "\n",
        "\n",
        "# display(df.shape[0])\n",
        "\n",
        "if False:\n",
        "    model = models.Word2Vec.load(model_path)\n",
        "else:\n",
        "    print(f\"Model build started: {datetime.now().time()}\")\n",
        "    model = models.Word2Vec(**config[\"word2vec_args\"])\n",
        "    model.build_vocab(df['clean'].tolist(), progress_per=10000)\n",
        "    model.train(\n",
        "        df['clean'].tolist(), total_examples=model.corpus_count, epochs=epochs, report_delay=1\n",
        "    )\n",
        "    # normalize the model\n",
        "    for k in model.wv.key_to_index:\n",
        "        model.wv[k] = model.wv[k] / np.linalg.norm(model.wv[k])    \n",
        "    model.save(model_path)\n",
        "    print(f\"Model build finished: {datetime.now().time()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI5ctN9mj3MW"
      },
      "source": [
        "import numpy as np\n",
        "for count, k in enumerate(model.wv.key_to_index):\n",
        "    if count > 5:\n",
        "        break\n",
        "    print(np.linalg.norm(model.wv[k]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rq5u99Tj4P6"
      },
      "source": [
        "from html import escape\n",
        "\n",
        "lemma = \"galû\"\n",
        "#lemma = \"kayyānu\"\n",
        "print(lemma)\n",
        "lemma_span = model.wv.most_similar(positive=[lemma], topn=30)\n",
        "lemma_span"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqAjM1CxkKX0"
      },
      "source": [
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "def get_sim_vector(s, t):\n",
        "    return np.array([np.dot(t, w) / (np.linalg.norm(t) * np.linalg.norm(w)) for w in s])\n",
        "\n",
        "def pay_attention(lemma_sentences, lemma):\n",
        "    vec = model.wv[lemma]\n",
        "    similarity_vector_list = []\n",
        "    attention_vector_list = []\n",
        "    attention_highlight_indices_list = []\n",
        "    attention_highlight_list = []\n",
        "    attention_list = []\n",
        "    for i, s in lemma_sentences.iterrows():\n",
        "        context = [w for w in s['context'] if w != lemma and w in model.wv.key_to_index]\n",
        "        context_vec = [model.wv[w] for w in context]\n",
        "        if(len(context_vec) == 0):\n",
        "            similarity_vector_list.append(None)\n",
        "            attention_vector_list.append(None)\n",
        "            attention_highlight_indices_list.append(None)\n",
        "            attention_highlight_list.append(None)\n",
        "            attention_list.append(None)\n",
        "        else:\n",
        "            sim_vec = get_sim_vector(context_vec, vec)\n",
        "            similarity_vector_list.append(sim_vec)\n",
        "            att_vec = softmax(sim_vec)\n",
        "            attention_vector_list.append(att_vec)\n",
        "            attention_highlight_indices = [x[0] for x in heapq.nlargest(3, enumerate(att_vec), key=lambda x: x[1])]\n",
        "            attention_highlight_indices_list.append(attention_highlight_indices)\n",
        "            attention_highlight_list.append(list(np.array(context)[attention_highlight_indices]))\n",
        "            assert len(context_vec) == len(att_vec), \"context vector and attention vector must have same length\"\n",
        "            attented_context_vec = np.array([att_vec[i] * context_vec[i] for i in range(len(att_vec))], dtype=tuple)\n",
        "            attention_list.append(np.sum(attented_context_vec, axis=0))\n",
        "    lemma_sentences['similarity_vector'] = similarity_vector_list\n",
        "    lemma_sentences['attention_vector'] = attention_vector_list\n",
        "    lemma_sentences['attention_highlight_indices'] = attention_highlight_indices_list\n",
        "    lemma_sentences['attention_highlight'] = attention_highlight_list\n",
        "    lemma_sentences['attention'] = attention_list\n",
        "\n",
        "lemma_sentences = df[df.apply(lambda x: lemma in x['clean'], axis=1)]\n",
        "context_window = 5\n",
        "context = [s[max(0, s.index(lemma) - context_window) : min(s.index(lemma) + context_window + 1, len(s))] for s in lemma_sentences['clean']]\n",
        "lemma_sentences['context'] = context\n",
        "pay_attention(lemma_sentences, lemma)\n",
        "lemma_sentences.dropna(inplace=True)\n",
        "display(lemma_sentences[['origin', 'sense', 'attention_highlight']])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caEc1jSrkSuN"
      },
      "source": [
        "!pip install kneed\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "from kneed import KneeLocator\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "n_neighbors = 5\n",
        "min_samples = 5\n",
        "\n",
        "def get_optimal_eps_knee(values):\n",
        "    nearest_neighbors = NearestNeighbors(n_neighbors=n_neighbors)\n",
        "    neighbors = nearest_neighbors.fit(values)\n",
        "    distances, indices = neighbors.kneighbors(values)\n",
        "    distances = np.sort(distances[:,n_neighbors-1], axis=0)\n",
        "    i = np.arange(len(distances))\n",
        "    knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    knee.plot_knee()\n",
        "    plt.xlabel(\"Points\")\n",
        "    plt.ylabel(\"Distance\")\n",
        "    #plt.savefig(\"knee.png\", dpi=300)\n",
        "    # print(distances[knee.knee])\n",
        "    return distances[knee.knee]\n",
        "\n",
        "def get_optimal_eps_heuristic(values):\n",
        "    best_eps = 0.95\n",
        "    for x in range(90, 0, -5):\n",
        "        eps = x / 100\n",
        "        clustering = DBSCAN(eps=eps, metric='cosine').fit(values)\n",
        "        noise = np.count_nonzero(clustering.labels_ == -1) / len(clustering.labels_)\n",
        "        print(f'Portion of noise: {noise}')\n",
        "        print(f'Current eps: {eps}')\n",
        "        if noise <= 0.5:\n",
        "            best_eps = eps\n",
        "        else:\n",
        "            break\n",
        "    print(f'Best eps: {best_eps}')\n",
        "    return best_eps\n",
        "\n",
        "\n",
        "values = lemma_sentences['attention'].tolist()\n",
        "best_eps = 0.29 #get_optimal_eps_heuristic(values)\n",
        "clustering = DBSCAN(eps=best_eps, metric='cosine').fit(values)\n",
        "lemma_sentences['cluster'] = clustering.labels_\n",
        "clusters = sorted(set(clustering.labels_)) # - {-1})\n",
        "#result = lemma_sentences[['origin', 'sense', 'attention_highlight', 'cluster']][lemma_sentences.cluster != -1].sort_values(by=['cluster']).groupby('cluster').head(100)\n",
        "result = lemma_sentences[['origin', 'sense', 'attention_highlight', 'cluster']].sort_values(by=['cluster']).groupby('cluster').head(100)\n",
        "\n",
        "# clusters = sorted(set(clustering.labels_))\n",
        "# result = lemma_sentences[['origin', 'sense', 'attention_highlight', 'cluster']].sort_values(by=['cluster']).groupby('cluster').head()\n",
        "# latex_path = str(\n",
        "#     DRIVE_ROOT\n",
        "#     / \"df.tex\"\n",
        "# )\n",
        "# result.to_latex(latex_path)\n",
        "# origin = []\n",
        "# for r in result[['origin']].iterrows():\n",
        "#     origin.append(r[1].str.wrap(50)[0].replace('\\n', '<br>'))\n",
        "# result['origin'] = origin\n",
        "result.style.hide_index()\n",
        "display(result)\n",
        "print(f'Best eps: {best_eps}')\n",
        "print(f'Number of clusters: {len(clusters)}')\n",
        "print(f'Portion of noise: {np.count_nonzero(clustering.labels_ == -1) / len(clustering.labels_)}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
